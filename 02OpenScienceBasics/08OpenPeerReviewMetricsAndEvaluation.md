![](/Images/Icons/peer_review.png)

## 8. Révision Ouverte par les Pairs, Métriques et Évaluation ouvertes 

### De quoi parle t-on ? 

Être chercheur implique d'être en évalué en permanence. Le monde de la recherche est une « économie du prestige » dans lequel la valeur académique est fondée sur le niveau d'estime dont bénéficient un chercheur et ses contributions de la part de ses pairs, des décideurs et d'autres acteurs [Blackmore et Kandiko, 2011](https://doi.org/10/fqrkft). Dans ce chapitre, il sera par conséquent important d'établir une distinction entre l'évaluation d'un travail académique et l'évaluation du chercheur lui-même. Mais tant le chercheur que ses travaux sont évalués selon deux méthodes principales : l'évaluation par les pairs et les métriques. La première est qualitative, la seconde est quantitative.

L'évaluation par les pairs est fondamentalement utilisée pour évaluer des travaux de recherche. Il s'agit du mécanisme formel d'assurance qualité par lequel les manuscrits scientifiques (par exemple, les articles de revues, les livres, les demandes de subvention et les documents de conférence) sont soumis à l'examen d'autres chercheurs, dont le retour et les avis sont ensuite utilisés pour améliorer les travaux soumis et prendre une décision définitive quant à leur publication, l'octroi de subventions ou le temps de parole lors d'une conférence.

L'évaluation ouverte par les pairs peut s'entendre différemment selon les personnes ou les communautés de chercheurs et a été définie comme un "terme générique englobant un certain nombre de manières plus ou moins distinctes d'adapter les modèles de révision par les pairs aux objectifs de la science ouverte ([Ross-Hellauer, 2017](https://doi.org/10/gc5sjh)).

Ses deux principales caractéristiques sont : "les identités ouvertes", à savoir que les auteurs connaissent l'identité des évaluateurs et réciproquement (pas de révision en simple aveugle), et les "rapports ouverts", à savoir que les rapports de révision sont publiés en même temps que l'article de référence. Ces caractéristiques peuvent être associées, mais pas nécessairement et peuvent être complétées par d'autres innovations, comme la "participation ouverte" permettant à des membres d'une communauté élargie de participer également au processus d'évaluation par les pairs ; les "échanges ouverts" permettant une interaction à double sens entre auteur(s) et évaluateurs, et/ou entre "reviewers" sont autorisés et encouragés. Il existe enfin, la "pré-révision ouverte des manuscrits" selon laquelle les manuscrits sont rendus immédiatement disponibles préalablement à toutes procédures formelles de révision par les pairs (soit en interne dans le cadre d'un processus d'éditorialisation, soit en externe depuis un serveur de prépublication).

Une fois qu'elles ont été soumises à la révision par les pairs, les publications de recherche constituent souvent la première mesure de la valeur du travail d'un chercheur (d'où le leitmotiv : "Publier ou Périr"). Cependant, évaluer la qualité des publications est une tâche difficile et subjective. Bien que certains exercices d'évaluation globale comme le Cadre d'Excellence pour la Recherche du Royaume-Uni, utilisent l'examen par les pairs, l'évaluation générale est souvent basée sur des **mesure quantitatives** ou **métriques**, comme le nombre de citations produites par les publications (h-index) ou bien une appréciation du prestige de la revue dans laquelle elles ont été publiées (calculée par le Facteur d'Impact de la revue). La prééminence de ces métriques et la façon dont elles pourraient fausser l'incitation à publier ont été mises en évidence ces dernières années dans des déclarations telles que le manifeste de Leyde ([Leiden Manifesto](http://www.leidenmanifesto.org/) - [traduction fr sur le site du COSO](https://www.ouvrirlascience.fr/le-manifeste-de-leiden-pour-la-mesure-de-la-recherche/)) et la déclaration de San Francisco sur l'évaluation de la recherche ([San Francisco Declaration on Research Assessment (DORA](https://sfdora.org/)).

Ces dernières années, des « métriques alternatives » ([*altmetrics*](https://www.altmetric.com/)*)* sont devenues un sujet de débat pour évoquer une évaluation équilibrée des efforts de recherche destinée à compléter le calcul du nombre de citations par la prise en compte d'autres mesures de l'impact en ligne des résultats d'une recherche, comme les signets, les articles de blogs, les tweets, les likes, les partages, la couverture média et d'autres indices de ce type.

Le problème que posent toutes ces métriques est qu'elles sont la propriété d'entités commerciales (par exemple Clarivate Analytics et Elsevier) et fondées sur des systèmes propriétaires, ce qui peut conduire à des problèmes de transparence.

![](/Images/Icons/umbrella.png)

### Fondement 

#### La révision ouverte par les pairs

Dès le 17 ème siècle avec la Société Royale de Londres (1662) et l'Académie Royale des Sciences de Paris (1669), et à mesure que la communauté scientifique obtenait le privilège de s'évaluer elle-même plutôt que de s'en référer à l'Eglise, il a fallu de nombreuses années pour que la Science se dote d'un processus complet d'évaluation par les pairs.

L'évaluation par les pairs, en tant que mécanisme formel, est bien plus récente qu'on ne le suppose habituellement. A titre d'exemple, la revue *Nature* ne l'a introduite qu'en 1967. Bien que les enquêtes montrent que les chercheurs accordent une grande importance à l'évaluation par les pairs, ils trouvent aussi que le système pourrait mieux fonctionner. On reproche souvent au processus d'évaluation par les pairs de prendre trop de temps, d'être parfois de qualité inégale ou qu'il ne permet souvent pas de détecter des erreurs, et l'anonymat peut dissimuler des biais. L'évaluation ouverte par les pairs (OPR en anglais) vise donc à garantir plus de transparence et de participation aux processus formels et informels de la révision par les pairs.

Être « reviewer » donne aux chercheurs l'occasion de s'impliquer dans une recherche innovante, de se créer un réseau, de gagner en expertise, et d'améliorer ses propres compétences rédactionnelles.

C'est un élément crucial dans le contrôle qualité du travail académique.

Néanmoins, les chercheurs ne sont généralement pas formés à pratiquer la révision par les pairs. Même lorsque les chercheurs se déclarent à l'aise avec la révision par les pairs traditionnelle, les nombreuses formes de révision ouverte représentent de nouveaux défis et de nouvelles opportunités.

Dans la mesure où la révision ouverte par les pairs recoupe une grande variété de pratiques, il appartient aux auteurs ainsi qu'aux reviewers de prendre celle-ci en compte.

![](/Images/02%20Open%20Science%20Basics/02_open_peer_review.png)

Concernant l'évaluation, le système actuel de récompenses et les métriques appliquées à la Science et à la recherche universitaire ne sont pas (encore) alignés avec la science ouverte. Les métriques utilisées pour évaluer la recherche (par exemple, le facteur d'impact de la revue, l'indice h) ne mesurent pas - et dès lors ne récompensent pas - les pratiques de recherche ouverte.


L'activité de pratique de la révision en mode ouvert n'est pas forcément reconnue comme un travail universitaire dans les scénarios d'avancement professionnels (par exemple, dans bien des cas, les jurys de bourses ne considèrent même pas les révisions ouvertes les plus brillantes comme des productions académiques à part entière). En outre, plusieurs métriques servant à l'évaluation - notamment certains types d'indicateurs bibliométriques - ne sont pas aussi ouverts et transparents que la communauté le souhaiterait.

Dans ces circonstances, les pratiques de la science ouverte sont perçues dans le meilleur des cas comme un fardeau supplémentaire qui ne récompense pas le chercheur. Au pire, elles sont considérées comme compromettant notoirement les chances de financements ultérieurs, d'avancement ou de titularisation.

Un [rapport récent de la Commission Européenne (2017)](https://doi.org/10.2777/75255) reconnaît qu'il existe fondamentalement deux approches de mise en œuvre de la science ouverte et de la manière dont des leviers d'incitation et d'évaluation peuvent la soutenir :

1.  Promouvoir simplement le statu quo en encourageant davantage d'ouverture, en construisant des métriques appropriées et en quantifiant les résultats ;

2.  Expérimenter des pratiques alternatives de recherche et d'évaluation, miser sur les données ouvertes, la science citoyenne et l'éducation ouverte.


![](/Images/Icons/finish.png)


### Objectifs de la formation 

1.  Reconnaître les éléments-clés de la révision ouverte par les pairs, ses avantages et inconvénients potentiels.

2.  Comprendre les différences entre les types de métriques utilisées pour évaluer la recherche et les chercheurs.

3.  Prendre part au débat sur la manière dont le système d'évaluation affecte la conduite des travaux de recherche.

### Éléments clés 

![](/Images/Icons/brain.png)


### Connaissances

#### Révision ouverte par les pairs

Les références célèbres en matière de révision ouverte par les pairs incluent des revues scientifiques appartenant à des éditeurs comme Copernicus, Frontiers, Biomed Central, eLife et F1000research

La révision ouverte par les pairs, sous ses différentes formes, présente beaucoup d'avantage potentiels pour les reviewers et les auteurs :

* Une révision dont l'identité des auteurs est connue (non-aveugle) engage davantage la responsabilité des reviewers et réduit les risques de biais ou de conflits d'intérêt non déclarés.

* Les rapports ouverts de révision apportent un niveau supplémentaire d'assurance qualité, en permettant à une plus large communauté de passer les révisions au crible avant d'engager les processus de prise de décision.

* La combinaison des identités ouvertes et des rapports ouverts donne en théorie lieu à de meilleures révisions, dans la mesure où associer publiquement son nom à un travail universitaire ou voir son travail de révision exposé, encourage les reviewers à être plus rigoureux.

* Les identités ouvertes et les rapports ouverts apportent aux reviewers de la crédibilité auprès du public pour leur travail de révision, les motivent à s'engager dans cette activité cruciale et permet qu'un travail de révision soit cité dans d'autres publications et dans d'autres activités professionnelles en lien avec une promotion ou une titularisation.

* La participation ouverte pourrait permettre de surmonter les problèmes liés à la sélection des reviewers par les éditeurs (par exemple : biais, esprit de club ou choix élitistes). Pour les chercheurs en début de carrière qui ne sont pas encore invités à faire de la révision par les pairs, ces procédures ouvertes constitueraient tout particulièrement une opportunité de bâtir leur réputation de chercheur et d'acquérir des compétences de reviewer.

Il existe néanmoins d'éventuelles chausse-trappes auxquelles il faut être vigilant :

* L'identité ouverte met fin à l'anonymat des reviewers (simple-aveugle) ou des auteurs et des reviewers (double-aveugle), alors qu'il est traditionnellement utilisé pour contrecarrer les biais sociaux (bien qu'il y ait peu de preuves de l'efficacité de l'anonymat en la matière). Il est donc important que les reviewers interrogent constamment leurs présupposés afin de s'assurer que leurs jugements ne portent que sur la qualité du manuscrit et non le statut, l'histoire personnelle ou les affiliations de l'auteur/des auteurs. Il est recommandé aux auteurs d'en faire de même lorsqu'ils reçoivent les commentaires de leurs évaluateurs.

* Émettre ou recevoir une critique est un processus inévitablement chargé de réactions émotionnelles - La subjectivité des auteurs et des reviewers peut les amener à un accord ou un désaccord quant à la manière de présenter les résultats de la recherche et/ou les aspects à améliorer, modifier ou corriger. Avec des identités ouvertes et/ou des rapports ouverts, la transparence peut potentiellement exacerber ces difficultés. Il est par conséquent essentiel que les évaluateurs veillent à communiquer leur point de vue de manière claire et pondérée, pour avoir le maximum de chances que cette restitution soit admise comme valable par l'auteur/les auteurs.

* L'absence d'anonymat pour les reviewers induite par l'évaluation ouverte peut faire dévier le processus en dissuadant les évaluateurs de porter des critiques de fonds, particulièrement à l'encontre de collègues de statut supérieur au leur.

Enfin, compte tenu de ces questions, des reviewers potentiels seraient plus susceptibles de décliner le travail d'évaluation.

### Les métriques ouvertes

La [Déclaration de San Francisco sur l'Evaluation de la Recherche](https://sfdora.org/) (DORA) recommande de prendre ses distances avec l'évaluation menée par les revues et de prendre en considération tous types de résultats et d'utiliser en parallèle différents types de métriques et d'évaluation narrative. Cette déclaration a été signée par des milliers de chercheurs, d'institutions, d'éditeurs et de bailleurs de fonds, qui se sont aujourd'hui engagés à la mettre en pratique. Le [Manifeste de Leyde](http://www.leidenmanifesto.org/) fournit un mode d'emploi pour un usage responsable des métriques.

Concernant les Métriques alternatives (*altmetrics*), Priem et al. (2010) font remarquer qu'elles présentent les avantages suivants : elles augmentent plus rapidement que le nombre de citations ; elles peuvent jauger l'impact de résultats scientifiques autres que les publications (par exemple, jeux de données, code, protocoles, articles de blog, tweets, etc.) et elles peuvent fournir différentes mesures d'impact pour un même objet scientifique. La mise à jour des métriques alternatives présente un intérêt particulier pour les chercheurs débutants dont l'impact peut ne pas être reflété par un nombre significatif de citations, mais dont l'évolution de carrière dépend des évaluations positives. En outre, les métriques alternatives peuvent aider à repérer précocement une recherche influente ou des connexions potentielles à établir entre chercheurs. Un rapport récent produit par le Groupe d'Experts de la Commission Européenne sur les Métriques Alternatives (EC Expert Group on Altmetrics - Wilsdon et al. (European Commission), 2017) a identifié les défis des métriques alternatives, dont le manque de robustesse et la vulnérabilité à la manipulation. Le rapport rappelle la loi de Goodhart : «Lorsqu'une mesure devient un objectif, elle cesse d'être une bonne mesure » ; il pointe également le manque d'engagement sur les réseaux sociaux dans certaines disciplines ou sur certains territoires géographiques et le fait de devoir compter sur des entités commerciales pour la gestion des données sous-jacentes.

![](/Images/Icons/gears.png)

### Compétences

Exemples d'exercices

* Les participants travaillent par groupes de trois. Chacun rédige individuellement une révision d'un court texte académique. 

* Procéder à la révision d'un article déposé sur un serveur de prépublications.

* [Utiliser un service gratuit de bibliométrie ou de métrique alternative (par exemple [Impactstory](https://impactstory.org/) [Paperbuzz](https://paperbuzz.org/)[Altmetric bookmarklet](https://www.altmetric.com/products/free-tools/bookmarklet/)[Dimensions.ai](https://www.dimensions.ai/) pour analyser les métriques d'un article, puis écrire un court texte explicatif sur la façon exacte dont les différentes métriques utilisées par chacun de ces services sont calculées (c'est plus compliqué qu'il n'y paraît, c'est une gageure de trouver une documentation complète sur ces métriques même avec des services qui semblent les plus transparents.


![](/Images/Icons/questions.png)

### Questions, obstacles et idées fausses reçues 

Q: L'évaluation de la recherche est-elle équitable ?

R: L'évaluation de la recherche est aussi équitable que le sont les méthode et techniques qu'elle utilise. Les métriques traditionnelles et les métriques alternatives tentent de mesurer la qualité de la recherche en mesurant son impact en termes quantitatifs, ce qui peut se faire avec exactitude, sans que cela soit obligatoire.

![](/Images/Icons/output.png)

### Acquis de la formation 

1.  Les participants seront en mesure d'identifier des revues appliquant l'évaluation ouverte par les pairs.

2.  Les participants feront connaissance avec un large spectre de métriques, leurs avantages et leurs inconvénients.

![](/Images/Icons/magnifying_glass.png)


### Lectures complémentaires 

* Directorate-General for Research and Innovation (European Commission) (2017). Evaluation of Research Careers Fully Acknowledging Open Science Practices: Rewards, Incentives and/or Recognition for Researchers Practicing Open Science. [doi.org/10.2777/75255](https://doi.org/10.2777/75255)

* Hicks et al. (2015) Bibliometrics: The Leiden Manifesto for research metrics. [doi.org/10.1038/520429a](www.doi.org/10.1038/520429a), [leidenmanifesto.org](http://www.leidenmanifesto.org/)

* Peer Review the Nuts and Bolts (2012). A Guide for Early Career Researchers. [PDF](http://senseaboutscience.org/wp-content/uploads/2016/09/peer-review-the-nuts-and-bolts.pdf)


### Projets et initiatives

* Make Data Count. [makedatacount.org](https://makedatacount.org/)

* NISO Alternative Assessment Metrics (Altmetrics) Initiative. [niso.org](http://www.niso.org/standards-committees/altmetrics)

* Open Rev. [openrev.org](https://en.wikipedia.org/wiki/Open_Rev)

* OpenUP Hub. [openuphub.eu](https://www.openuphub.eu/review)

* Peer Reviewers' Openness Initiative. [opennessinitiative.org](https://opennessinitiative.org/)

* Peerage of Science. Un service gratuit de révision par les pairs et de publication. [peerageofscience.org](https://www.peerageofscience.org/)

* Responsible Metrics. [responsiblemetrics.org](https://responsiblemetrics.org/)

* Snowball Metrics. Des métriques de recherche standardisées -- par le secteur pour le secteur. [snowballmetrics.com](https://www.snowballmetrics.com/)
